\section{Conclusion}
\label{section:conclusion}
\normalsize
The last-mile search problem involves finding locations matching some set of spatial constraints that are not easily query-able with a GIS tool.
This typically includes visual landmarks or terrain features that comprise a partial or uncertain set of information about the location of interest.
Solving the last-mile search problem without manual inspection of images requires spatial search techniques beyond just searching by geographic proximity (i.e. nearest neighbor searching).
Inspired by the way humans recall and search for information, we developed \emph{GESTALT}, a pipeline that (to our knowledge) is the first to enable querying for locations based on the spatial configuration of nearby objects given partial information about them.
We automatically detect objects from geotagged images, fuzzily assign objects to (possibly multiple) locations, enable pictorial querying on spatial relations between objects, and perform probabilistic search over locations, returning partial matches when the search constraints are too narrow, and ranked results when they are too broad.
\emph{GESTALT} shows high recall on the ground truth benchmark dataset we contribute and easily scales to the larger, noisier datasets we tested it on.
This work invites new avenues of research in improving human-centric spatial search.
Advances in computer vision can be leveraged to adjust spatial search for uncertainty in object position within an image scene using the camera's bearing information~\cite{Ming2021,Liu2020,Snavely2011,Hays2008}.
Techniques can also be developed to handle querying across additional dimensions, like time, object attribute values, and object size, scale, and quantity.
